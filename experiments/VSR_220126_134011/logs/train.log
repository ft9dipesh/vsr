22-01-26 13:40:11.873 - INFO:   name: VSR
  phase: train
  gpu_ids: [0, 1]
  path:[
    log: experiments\VSR_220126_134011\logs
    tb_logger: experiments\VSR_220126_134011\tb_logger
    results: experiments\VSR_220126_134011\results
    checkpoint: experiments\VSR_220126_134011\checkpoint
    resume_state: None
    experiments_root: experiments\VSR_220126_134011
  ]
  datasets:[
    train:[
      name: REDS
      dataroot_GT: D:\Development\dm-vsr\data\dataset\train_sharp_wval.lmdb
      dataroot_LQ: D:\Development\dm-vsr\data\dataset\train_sharp_bicubic_wval.lmdb
      mode: REDS
      n_frames: 5
      data_type: lmdb
      scale: 4
      GT_size: 256
      LQ_size: 64
      batch_size: 8
      num_workers: 16
      use_shuffle: True
      interval_list: [1]
      random_reverse: False
      border_mode: False
      use_flip: True
      use_rot: True
    ]
    val:[
      name: REDS4
      datatype: lmdb
      data_len: 3
    ]
  ]
  model:[
    which_model_G: sr3
    finetune_norm: False
    unet:[
      in_channel: 6
      out_channel: 3
      inner_channel: 64
      norm_groups: 16
      channel_multiplier: [1, 2, 4, 8, 16]
      attn_res: []
      res_blocks: 1
      dropout: 0
    ]
    beta_schedule:[
      train:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
      val:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
    ]
    diffusion:[
      image_size: 1034
      channels: 3
      conditional: True
    ]
  ]
  train:[
    n_iter: 1000000
    val_freq: 10000.0
    save_checkpoint_freq: 10000.0
    print_freq: 50
    optimizer:[
      type: adam
      lr: 3e-6
    ]
  ]
  distributed: True

22-01-26 13:40:11.900 - INFO: Temporal augmentation interval list: [1], with random reverse is False.
22-01-26 13:40:11.923 - INFO: Using lmdb meta info for cache keys.
22-01-26 13:40:11.939 - INFO: Dataset [REDSDataset - REDS] is created.
